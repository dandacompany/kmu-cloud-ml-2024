{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 처리 및 분석\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# 머신러닝\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# AWS 관련\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter, CategoricalParameter\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "\n",
    "# 기타 유틸리티\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker 세션 및 역할 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session = boto3.Session(profile_name='awstutor')\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3_session)\n",
    "role = os.environ.get('SAGEMAKER_EXECUTION_ROLE_ARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 데이터 저장 위치 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_training_file_location :  s3://dante-sagemaker/cifar10/input/training/\n",
      "s3_validation_file_location :  s3://dante-sagemaker/cifar10/input/validation/\n",
      "s3_output_location :  s3://dante-sagemaker/cifar10/output/\n",
      "s3_checkpoint_location :  s3://dante-sagemaker/cifar10/checkpoint/\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'dante-sagemaker'\n",
    "project_name = 'cifar10'\n",
    "\n",
    "s3_training_file_location = r's3://{0}/{1}/input/training/'.format(bucket_name, project_name)\n",
    "s3_validation_file_location =r's3://{0}/{1}/input/validation/'.format(bucket_name, project_name)\n",
    "\n",
    "s3_output_location = r's3://{0}/{1}/output/'.format(bucket_name, project_name)\n",
    "s3_checkpoint_location = r's3://{0}/{1}/checkpoint/'.format(bucket_name, project_name)\n",
    "\n",
    "print('s3_training_file_location : ', s3_training_file_location)\n",
    "print('s3_validation_file_location : ', s3_validation_file_location)\n",
    "print('s3_output_location : ', s3_output_location)\n",
    "print('s3_checkpoint_location : ', s3_checkpoint_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 다운로드 및 s3 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 데이터 저장\n",
    "os.makedirs(f'dataset/{project_name}', exist_ok=True)\n",
    "np.save(f'dataset/{project_name}/x_train.npy', x_train)\n",
    "np.save(f'dataset/{project_name}/y_train.npy', y_train)\n",
    "np.save(f'dataset/{project_name}/x_test.npy', x_test)\n",
    "np.save(f'dataset/{project_name}/y_test.npy', y_test)\n",
    "\n",
    "s3_train_features = os.path.join(s3_training_file_location, 'x_train.npy')\n",
    "s3_train_labels = os.path.join(s3_training_file_location, 'y_train.npy')\n",
    "s3_validation_features = os.path.join(s3_validation_file_location, 'x_test.npy')\n",
    "s3_validation_labels = os.path.join(s3_validation_file_location, 'y_test.npy')\n",
    "\n",
    "wr.s3.upload(f'dataset/{project_name}/x_train.npy', s3_train_features, boto3_session=boto3_session)\n",
    "wr.s3.upload(f'dataset/{project_name}/y_train.npy', s3_train_labels, boto3_session=boto3_session)\n",
    "wr.s3.upload(f'dataset/{project_name}/x_test.npy', s3_validation_features, boto3_session=boto3_session)\n",
    "wr.s3.upload(f'dataset/{project_name}/y_test.npy', s3_validation_labels, boto3_session=boto3_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 스크립트 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'scripts/{project_name}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/cifar10/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/cifar10/train.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    x_train = np.load(os.path.join('/opt/ml/input/data/train', 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join('/opt/ml/input/data/train', 'y_train.npy'))\n",
    "    x_test = np.load(os.path.join('/opt/ml/input/data/test', 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join('/opt/ml/input/data/test', 'y_test.npy'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test, batch_size, epochs, learning_rate, model_dir):\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # 모델 저장\n",
    "    model.save(os.path.join(model_dir, 'model'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--epochs', type=int, default=20)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.001)\n",
    "    parser.add_argument('--model_dir', type=str, default='/opt/ml/model')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = load_data()\n",
    "    \n",
    "    model(x_train, y_train, x_test, y_test, args.batch_size, args.epochs, args.learning_rate, args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Estimator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point=f'scripts/{project_name}/train.py',\n",
    "                          role=role,\n",
    "                          sagemaker_session=sagemaker_session,\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m5.xlarge',\n",
    "                          framework_version='2.4.1',\n",
    "                          py_version='py37',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={\n",
    "                              'epochs': 20\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터 튜닝 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 범위 설정\n",
    "hyperparameter_ranges = {\n",
    "    'batch-size': IntegerParameter(32, 128),  # 배치 크기: 32에서 128 사이의 정수\n",
    "    'learning-rate': ContinuousParameter(0.0001, 0.01)  # 학습률: 0.0001에서 0.01 사이의 연속값\n",
    "}\n",
    "\n",
    "# 목표 지표 설정\n",
    "objective_metric_name = 'val_accuracy'  # 검증 정확도를 목표 지표로 사용\n",
    "objective_type = 'Maximize'  # 목표 지표를 최대화하려고 함\n",
    "\n",
    "# 지표 정의 설정\n",
    "metric_definitions = [{'Name': 'val_accuracy',\n",
    "                       'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]  # 로그에서 검증 정확도를 추출하기 위한 정규 표현식\n",
    "\n",
    "# 하이퍼파라미터 튜너 설정\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=tf_estimator,  # 사용할 estimator\n",
    "    base_tuning_job_name=f'{project_name}-tuner',  # 튜닝 작업의 기본 이름\n",
    "    strategy='Bayesian',  # 베이지안 최적화 전략 사용 (다른 옵션: Random)\n",
    "    objective_metric_name=objective_metric_name,  # 최적화할 목표 지표\n",
    "    hyperparameter_ranges=hyperparameter_ranges,  # 탐색할 하이퍼파라미터 범위\n",
    "    metric_definitions=metric_definitions,  # 지표 정의\n",
    "    max_jobs=20,  # 최대 튜닝 작업 수\n",
    "    max_parallel_jobs=3,  # 동시에 실행할 최대 작업 수\n",
    "    objective_type=objective_type  # 목표 지표의 최적화 방향 (최대화)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "tuner.fit({'train': s3_training_file_location, 'test': s3_validation_file_location})\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적 하이퍼파라미터로 모델 엔드포인트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = tuner.best_estimator()\n",
    "best_estimator.fit({'train': s3_training_file_location, 'test': s3_validation_file_location})\n",
    "\n",
    "# 최종 모델 배포 및 평가\n",
    "predictor = best_estimator.deploy(initial_instance_count=1, instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "y_pred = predictor.predict(np.expand_dims(x_test, axis=-1))\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'테스트 세트 정확도: {accuracy:.4f}')\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "print('혼동 행렬:')\n",
    "print(confusion_mtx)\n",
    "\n",
    "# 분류 보고서 출력\n",
    "print('\\n분류 보고서:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엔드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
